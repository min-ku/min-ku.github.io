<!doctype html>
<html>
    <head>
        <meta charset="utf-8" />
        <meta name="description" content="Minku Kim" />
        <meta name="keywords" content="Robotics Legged-Locomotion Learning" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>Research Projects</title>
        <link
            href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
            rel="stylesheet"
        />

        <link rel="stylesheet" href="./static/css/bulma.min.css" />
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
        <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
        />
        <link rel="stylesheet" href="./static/css/index.css" />
        <link rel="icon" href="./images/icon.jpeg" />

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <script src="./static/js/index.js"></script>

        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script
            type="text/javascript"
            id="MathJax-script"
            async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
        ></script>
    </head>
    <body>
        <header class="header">
            <div class="container">
                <div class="columns is-centered">
                    <div class="column is-8"></div>
                </div>
            </div>
        </header>
        <section class="section">
            <div id="research" class="container is-max-desktop">
                <!-- Abstract. -->
                <div class="columns is-centered" style="margin-top: 3em; margin-bottom: 5em;">
                    <h2 class="title is-1">Research</h2>
                </div>
                <div class="columns is-centered">
                    <div class="column is-4">
                        <div class="content" style="justify-content: center; align-items: center; display: flex;">
                            <video style="height: 15em; width: 100%;" controls autoplay loop muted>
                                <source src="videos/humanoidhanoi.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                                Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement
                            </h3>
                            <p>
                                In this work, We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level.
                                In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition,
                                in contrast to non-shared designs that use separate low-level controllers per skill. Additionally, we introduce Humanoid Hanoi,
                                a long-horizon Tower-of-Hanoi box rearrangement benchmark to evaluate long-horizon performance, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous
                                rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.
                                <br><em>
                                    <span style="color:red;">Under Review</span>
                                </em><br>
                                <strong><a href="https://arxiv.org/abs/2602.13850" target="_blank"> [Arxiv Link]</a></strong>
                            </p>
                        </div>
                    </div>
                </div>


                <div class="columns is-centered">
                    <div class="column is-4">
                        <div class="content" style="justify-content: center; align-items: center; display: flex;">
                            <video style="height: 15em; width: 100%;" controls autoplay loop muted>
                                <source src="videos/sage.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                                SAGE: Semantic And Geometric Estimation of 6D Object Pose from Multi-View Observations
                            </h3>
                            <p>
                                We introduce SAGE, a unified multi-view transformer that bridges the gap between geometric scene reconstruction and semantic object understanding.
                                SAGE utilizes a cross-view attention mechanism to enforce 3D geometric consistency across multiple camera angles.
                                By integrating CLIP embeddings, SAGE enables open-vocabulary object
                                grounding from text queries without task-specific retraining. The architecture incorporates a differentiable Direct Linear
                                Transform (DLT) for camera estimation and a differentiable Non-Maximum Suppression (NMS) layer to resolve geometric symmetries.
                                On an NVIDIA GeForce RTX 3090 Ti, SAGE achieves up to 18 Hz for single-view and over 10 Hz for two-view 256x256 inputs,
                                providing an efficient and modular perception framework for robotic manipulation.

                                <br><em>
                                    <span style="color:red;">Under Review</span>
                                </em><br>
                            </p>
                        </div>
                    </div>
                </div>


                <div class="columns is-centered">
                    <div class="column is-4">
                        <div class="content" style="justify-content: center; align-items: center; display: flex;">
                            <video style="height: 15em; width: 100%;" controls autoplay loop muted>
                                <source src="videos/asm.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                                ASM-6D: Real-Time 6D Object Pose and Shape Estimation via Active Shape Models and ADMM
                            </h3>
                            <p>
                                We present ASM-6D, a framework for real-time category-level 6D pose, shape, and scale estimation from partial RGB-D observations.
                                We bridge this gap by formulating a unified optimization problem over the SIM(3) manifold and introducing a high-throughput Matrix ADMM solver.
                                Our approach concurrently estimates an object’s pose and shape at frequencies exceeding 100 Hz, with sub-linear
                                scaling with respect to batch size and active shape model complexity. For learning-based keypoint detector frontends,
                                we provide a pipeline for generating geometrically consistent keypoints, enabling the training of robust detection models
                                without manual labeling. We demonstrate the system's reliability through global optimality certification via SDP duality
                                and show resilience to both stochastic noise and occlusions. ASM-6D provides a mathematically rigorous and computationally
                                efficient foundation for real-time, closed-loop robotic manipulation.

                                <br><em>
                                    <span style="color:red;">Under Review</span>
                                </em><br>
                            </p>
                        </div>
                    </div>
                </div>


                <div class="columns is-centered">
                    <div class="column is-4">
                        <div class="content" style="justify-content: center; align-items: center; display: flex;">
                            <video style="height: 15em; width: 100%;" controls autoplay loop muted>
                                <source src="videos/pose.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                                Dynamic-ASM6D: Real-time 6D Object Pose and Shape Estimation via Active Shape Models and ADMM
                            </h3>
                            <p>
                                This research introduces a real-time framework for simultaneous 6D object pose tracking and shape estimation in dynamic, real-world environments.
                                The system combines Active Shape Models with ADMM-based point cloud registration to handle occlusions, deformations, and shape variability without requiring CAD models.
                                A novel SVGD-based multi-hypothesis tracker over SIM(3) ensures robustness under symmetry and noise. Running at 100–200Hz on GPU,
                                this framework enables reliable object-centric perception and closed-loop manipulation in fast-changing, human-centric tasks.

                                <br><strong>
                                    Accepted to <a href="https://equisystems.github.io/index.html#accepted" target="_blank">
                                    Equivariant Systems: Theory and Applications in State Estimation, Artificial Intelligence and Control Workshop at RSS 2025</a>
                                </strong><br>
                                <strong>
                                    Accepted to <a href="https://www.tcoptrob.org/news/2025-07-21/" target="_blank">
                                    TC Virtual Poster Session and Networking Event 2025</a>
                                </strong>
                            </p>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-4">
                        <div class="content" style="justify-content: center; align-items: center; display: flex;">
                            <video style="height: 15em; width: 100%;" controls autoplay loop muted>
                                <source src="videos/cassie_walking.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                                Learning a Vision-Based Footstep Planner for Hierarchical Walking Control
                            </h3>
                            <!-- <p>
                                By Minku Kim, Brian Acosta, Pratik Chaudhari and Michael Posa 
                            </p> -->
                            <p>
                                This research presents a vision-based hierarchical control framework to enhance locomotion in unstructured terrains.
                                The framework integrates a reinforcement learning (RL) footstep planner, which generates adaptive footsteps from a robot-centric
                                elevation map and the Angular Momentum Linear Inverted Pendulum model as policy input,
                                with a low-level Operational Space Controller (OSC) that tracks the planned trajectories.
                                We validate our approach on the underactuated bipedal robot Cassie and show evaluations across diverse terrain conditions
                                in both simulation and hardware experiments.
                                <br><strong>
                                    Accepted to <a href="https://2025humanoids.org/" target="_blank">
                                    2025 IEEE-RAS 24th International Conference on Humanoid Robots <span style="color:red;">[Oral Presentation]</span></a>
                                </strong>
                                <br><strong><a href="https://arxiv.org/abs/2508.06779" target="_blank"> [Arxiv Link]</a></strong>
                            </p>
                        </div>
                    </div>
                </div>
                <!-- Show More Button -->
                <div class="columns is-centered" style="margin-top: 2em;">
                    <div class="column has-text-centered">
                        <button class="button is-small is-link is-light" onclick="toggleOldProjects()">Show Older Research</button>
                    </div>
                </div>

                <!-- Wrap older research in this container -->
                <div id="old-research" style="display: none;">
                <div class="columns is-centered">
                    <div class="column is-4">
                        <div class="content" style="justify-content: center; align-items: center; display: flex;">
                            <img src="images/forest.jpg" style="height: 15em; width: 100%;" />
                        </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                            Terrain Recognition System for Wearable Device
                            </h3>
                            <p>
                            This research develops a terrain recognition algorithm for a wearable system to support the mobility of forestry workers.
                            Utilizing a single stereo camera, the system identifies safe regions and classifies ground types in dense forest environments,
                            enabling adaptive operational mode switches for the wearable device in response to varying terrain types.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-4">
                    <div class="content" style="justify-content: center; align-items: center; display: flex;">
                        <img src="images/fault.png" style="height: 15em; width: 100%;" />
                    </div>
                    </div>
                    <div class="column">
                        <div class="content is-small" style="margin-top: 0em;">
                            <h3 class="title is-5" style="margin-bottom: 0.3em;">
                            AI-based Real-Time Monitoring and Fault Diagnosis for Gear Failure in Electric Vehicle Reducers
                            </h3>
                            <p>
                            This research constructs a real-time fault diagnosis model for monitoring gear damage in electric vehicle (EV) reducers
                            using learning-based methods and demonstrates 98% detection. The study utilized feature extraction methods,
                            including Wavelet Packet Decomposition (WPD), Mel-Frequency Cepstral Coefficients (MFCC),
                            STFT-spectrogram, and Mel-spectrum features to train a Convolutional Neural Network (CNN) model.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-4">
                    <div class="content" style="justify-content: center; align-items: center; display: flex;">
                        <video style="height: 15em; width: 100%;" controls autoplay loop muted>
                        <source src="videos/UAV.mp4" type="video/mp4">
                        </video>
                    </div>
                    </div>
                    <div class="column">
                    <div class="content is-small" style="margin-top: 0em;">
                        <h3 class="title is-5" style="margin-bottom: 0.3em;">
                        Vision-based UAV control System for Following Drones in Complex Visual Environments
                        </h3>
                        <p>
                        This research develops a real-time (>30fps) vision-based UAV control system that combines a YOLO-based object detection algorithm with a
                        Siamese network for tracking and following dynamic drones in complex visual environments.
                        The project addresses critical challenges in small object detection under limited visibility
                        by implementing a custom small-object centric loss function and contextual region refinement methodology.
                        </p>
                    </div>
                    </div>
                </div>
                </div>
        </section>

        <section class="section">

                            </div>
        </section>
        <footer class="footer">
            <div class="container is-max-desktop">
                <div class="column is-centered">
                    <span class="link-block">
                        <a
                            href="index.html"
                            class="external-link is-normal is-rounded is-dark"
                        >
                            <span class="icon">
                                <i class="fa fa-home"></i>
                            </span>
                            <span>Home</span>
                        </a>
                    </span>
                </div>
            </div>
        </footer>
    </body>
    <script>
        function toggleOldProjects() {
          const section = document.getElementById("old-research");
          const button = event.target;
          if (section.style.display === "none") {
            section.style.display = "block";
            button.textContent = "Hide Older Research";
          } else {
            section.style.display = "none";
            button.textContent = "Show Older Research";
          }
        }
    </script>
</html>
